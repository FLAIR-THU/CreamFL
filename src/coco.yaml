dataloader:
  batch_size: 64
  eval_batch_size: 64
  num_workers: 16
  crop_size: 224
  word_dim: 300
  random_erasing_prob: 0.2
  caption_drop_prob: 0.1

model:
  name: pcme
  embed_dim: 2048  # (ignored value, set in code)  2048 origin
  cnn_type: resnet101  # (ignored value, set in code) res152 origin
  wemb_type: glove
  word_dim: 300
  cache_dir: /data/mmdata/log/server
  n_samples_inference: 7
  eval_method: matmul
  use_img_client: True
  use_txt_client: True
  use_mm_client: True
  client: 0.1
  momentum: 0
  sample: 0
  sample_way: dis
  img_iid: 0
  txt_iid: 0

# optimizer configuration
optimizer:
  name: adamp
  learning_rate: 0.0002
  weight_decay: 0.0

# lr scheduler configuration
lr_scheduler:
  name: cosine_annealing
  T_max: 30

# criterion configuration
criterion:
  name: pcme
  init_negative_scale: 15
  init_shift: 15
  num_samples: 7
  vib_beta: 0
  criterion__negative_sampling_ratio: -1

# detailed training configuration
train:
  model_save_path: model_last_no_prob.pth
  best_model_save_path: model_best_no_prob.pth
  finetune_epochs: 30 # not used
  finetune_lr_decay: 0.1
  log_step: 100
  grad_clip: 2
  val_epochs: 10 # not used
  use_fp16: False
  output_file: model_noprob.log

vqa_hidden_sizes: [1024,1024]
vqa_dropout: 0.0  # (ignored value, set in code)